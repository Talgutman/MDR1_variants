{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af0b220-7b98-497f-927e-022b705c4741",
   "metadata": {},
   "source": [
    "In this notebook we find positions with significantly low/fast CAI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5e590-6efd-4c7f-98bd-5fd64d99792a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18ee9d1-0764-4032-8d58-b9e15e56eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pickle\n",
    "import io\n",
    "from Utils_Tal import SynonymousCodons, AAs, reverse_dict, aa_positions_to_nt_positions\n",
    "\n",
    "from Bio import AlignIO\n",
    "from Bio import SeqIO\n",
    "import concurrent.futures\n",
    "import scipy.stats\n",
    "import statsmodels.stats.multitest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5e9a3-32e3-455b-b6ea-e8d15a8e7b5f",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2cd4eb9-971f-48f1-b2bd-c4a6cf000cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' In this function we will create a single dictionary holding dictionaries with the CAI weights per\n",
    "organism, for all our orthologs'''\n",
    "def create_CAI_dict_all_organisms(codon_usage_path:str) -> dict:\n",
    "    weight_dicts_all_organisms = {}\n",
    "    for filename in os.listdir(codon_usage_path):\n",
    "        if filename.endswith(\".pickle\"):\n",
    "            weight_dict = pd.read_pickle(codon_usage_path + filename)\n",
    "            organism = filename.split(\".\")[0]\n",
    "            weight_dicts_all_organisms[organism] = weight_dict\n",
    "    return(weight_dicts_all_organisms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb810aa6-b015-4c66-9bbf-29a906333a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function recieves the alignment in nucleotides and returns the alignment in codons'''\n",
    "def get_original_codons_array(alignment_NT) -> np.ndarray:\n",
    "    #create codon array from nucleotide array\n",
    "    nuc_array = np.array(alignment_NT) # an array where each column is a nucleotide position\n",
    "    #lets merge every three columns into one, to obtain a numpy of codon positions instead of nucleotide positions\n",
    "    codons_array = np.zeros((nuc_array.shape[0],nuc_array.shape[1]//3),dtype=object)\n",
    "    for col in range(0,nuc_array.shape[1],3):\n",
    "        merged_to_codon = np.char.add(np.char.add(nuc_array[:,col],nuc_array[:,col+1]),nuc_array[:,col+2]) \n",
    "        codons_array[:,col//3] = merged_to_codon\n",
    "    return(codons_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66051c0f-59bc-4cc4-a807-157bb7099cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function recreates the randomizations. In order to save memory we only saved the original msa \n",
    "and a dictionary with the location of the differences of each randomization and the original msa. \n",
    "There are two keys in the dictionary:\n",
    "1. 'locations': the locations of the differences in the msa\n",
    "2. 'codons': the codons in the randomization in these locations'''\n",
    "\n",
    "def get_randomized_codons_array(randomization_path:str, original_codons_array:np.ndarray, num_rands:int) -> np.ndarray:\n",
    "    diff_dict = pd.read_pickle(randomization_path)\n",
    "    rand = original_codons_array.copy()\n",
    "    rand = np.tile(rand, (num_rands, 1, 1))\n",
    "    rand[diff_dict['locations']] = diff_dict['codons']\n",
    "    return(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38bac116-7666-4f23-aa02-acf1cd4581fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function maps between the row index in the msa and the organism which it stands for''' \n",
    "def create_ind_to_name_dict(path_NT:str) -> dict:\n",
    "    ind_to_name = {}\n",
    "    records = SeqIO.parse(open(path_NT),'fasta')\n",
    "    for record in records:\n",
    "        index = int(record.description.split(\"_\")[0])\n",
    "        name = \"_\".join(record.description.split(\"_\")[1:])\n",
    "        ind_to_name[index] = name\n",
    "    return(ind_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4a063d-eaef-4157-bee8-0075074d5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function creates the CAI matrices by mapping a codon to its eqivalent CAI weight.'''\n",
    "def from_codon_to_CAI(codons_list:list, slice_cur_ortho:np.ndarray, weight_dict:dict) -> np.ndarray:\n",
    "    #initilize results matrix\n",
    "    res = np.empty(slice_cur_ortho.shape)\n",
    "    res[:] = np.nan\n",
    "    for cur_codon in codons_list:\n",
    "        if cur_codon in weight_dict.keys():\n",
    "            res[slice_cur_ortho == cur_codon] = weight_dict[cur_codon]\n",
    "    return(res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e3f310-ef32-4ffe-b614-4dcedd4d75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This functions returns the z-scores and respective p-values for each CDS position. '''\n",
    "def get_scores(original_scores: np.ndarray, random_scores: np.ndarray, good_mask: np.ndarray) -> pd.DataFrame:\n",
    "    \n",
    "    #get the parameters of the normal distribution\n",
    "    miu = np.mean(random_scores, axis = 0)\n",
    "    sigma = np.std(random_scores, axis = 0)\n",
    "    \n",
    "    #get the z-scores\n",
    "    z_scores = (original_scores - miu) / sigma\n",
    "    \n",
    "    #get the one-sided p-values \n",
    "    p_vals = scipy.stats.norm.sf(abs(z_scores))\n",
    "    # correct FDR. we have nan values (for positions where the randomizations are the same and so sigma = 0) and the FDR correction function doesnt work with nans, \n",
    "    # so we will use a mask\n",
    "    mask = np.isfinite(p_vals) #mask contains \"True\" only the non-nan positions\n",
    "    pval_corrected = np.empty(p_vals.shape) #initilize the final result in the right dimensions\n",
    "    pval_corrected.fill(np.nan) #fill it with nans\n",
    "    pval_corrected[mask] = statsmodels.stats.multitest.multipletests(p_vals[mask],method='fdr_bh')[1] #insert the corrected p-vals at the non-nan positions\n",
    "    \n",
    "    res_df = pd.DataFrame(columns = [\"z-score\", \"p-value\", \"corrected p-value\", \"good_position\"])\n",
    "    res_df.index.name='position of CDS'\n",
    "    res_df[\"z-score\"] = z_scores\n",
    "    res_df[\"p-value\"] = p_vals\n",
    "    res_df[\"corrected p-value\"] = pval_corrected\n",
    "    res_df[\"good_position\"] = good_mask\n",
    "\n",
    "    return(res_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f267d1de-723e-4e8f-8d5d-824ea3bf67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function calls our main function \"get_significant_cai_positions_single_gene\" for a batch of genes'''\n",
    "def do_for_single_batch(single_batch_genes: list, num_randomizations:int) -> None:\n",
    "    for gene in single_batch_genes:\n",
    "        get_cai_zscores_single_gene(gene, num_randomizations)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "516d63b1-a30a-4756-bdbe-8df6cf1b1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes the original and randomized versions of the msa of a single gene, creates cai\n",
    "matrices according to the msas, and finds positions with significant low/high cai scores. \n",
    "It saves both the cai scores of each of the positions and the significant positions. '''\n",
    "def get_cai_zscores_single_gene(gene:str, num_randomizations:int) -> None:\n",
    "    \n",
    "    try:\n",
    "        ''' (1) Download data'''\n",
    "        #original MSA in nucleotides - unzip and read alignment\n",
    "        path_NT = f\"../co_trans_data/orthologs/nt_after_msa/{gene}.fasta.gz\" #MSA in nucleotides\n",
    "        local_path = f\"./{gene}.fasta\" #save to machine we are running on \n",
    "        !gzip -c -d $path_NT > $local_path #unzip localy, without erasing the zipped version\n",
    "        alignment_NT = AlignIO.read(local_path, \"fasta\") #read the alignement\n",
    "        codons_array = get_original_codons_array(alignment_NT) #from nt to codons\n",
    "        \n",
    "        #get column and vertical randomizations\n",
    "        path_col_rand = f\"../Results/AllGenes/column_permutations/{gene}.pickle.gz\" #MSA in nucleotides\n",
    "        codons_array_col = get_randomized_codons_array(path_col_rand, codons_array, num_randomizations)\n",
    "        path_ver_rand = f\"../Results/AllGenes/vertical_permutations/{gene}.pickle.gz\" #MSA in nucleotides\n",
    "        codons_array_ver = get_randomized_codons_array(path_ver_rand, codons_array, num_randomizations)\n",
    "\n",
    "        '''Remove MSA positions where the human ortholog has indels (these positions do not interest us\n",
    "        in this project and we want our coordinates system to be the same as the human cds) '''\n",
    "        human_indels_locs = np.where(codons_array[0,:] == '---')[0]\n",
    "        codons_array = np.delete(codons_array, human_indels_locs, axis = 1)\n",
    "        codons_array_col = np.delete(codons_array_col, human_indels_locs, axis = 2)\n",
    "        codons_array_ver = np.delete(codons_array_ver, human_indels_locs, axis = 2)\n",
    "\n",
    "        '''Change from codons to CAI weights. Keep in mind that the weights are different for each ortholog. \n",
    "        First, we create CAI weight matrices. These matrices have nans in the indel positions and also in \n",
    "        positions of codons with ambigious alphabet. All other cells have CAI weights according to the \n",
    "        specific ortholog's codon usage biases'''\n",
    "        num_randomizations, num_orthologs, num_codons =  codons_array_col.shape\n",
    "        \n",
    "        #initilize CAI weight matrices\n",
    "        orig_CAI = np.empty(codons_array.shape)\n",
    "        orig_CAI[:] = np.nan\n",
    "        col_CAI = np.empty(codons_array_col.shape)\n",
    "        col_CAI[:] = np.nan\n",
    "        ver_CAI = np.empty(codons_array_ver.shape)\n",
    "        ver_CAI[:] = np.nan\n",
    "\n",
    "        '''the keys in the \"weight_dicts_all_organisms\" dictionary are the organisms names. But when we\n",
    "        iterate over our codon matrices *we dont have* the organisms names. However, the organisms are ordered\n",
    "        in the matrix as they are ordered in the alignement. So, we will get the fasta file that contains the\n",
    "        CDSs of our gene and create a dictinary: dict[matrix_row_index] = organism_name.'''\n",
    "        ind_to_name = create_ind_to_name_dict(local_path)\n",
    "        os.remove(local_path)\n",
    "\n",
    "        for cur_ortho in range(num_orthologs): #iterate over orthologs\n",
    "            cur_ortho_name = ind_to_name[cur_ortho] #current ortholog name\n",
    "            cur_ortho_weights = CAI_all_organisms[cur_ortho_name] #a dictionary with the weights of the current ortholog\n",
    "\n",
    "            #get the parts of the codon matrices that are of the current ortholog\n",
    "            orig_cur_ortho = codons_array[cur_ortho,:]\n",
    "            col_cur_ortho = codons_array_col[:,cur_ortho,:] \n",
    "            ver_cur_ortho = codons_array_ver[:,cur_ortho,:]\n",
    "\n",
    "            codons_list = list(reverse_dict.keys()) #list of all codons\n",
    "\n",
    "            #update CAI weight matrices\n",
    "            orig_CAI[cur_ortho,:] = from_codon_to_CAI(codons_list, orig_cur_ortho, cur_ortho_weights)\n",
    "            col_CAI[:,cur_ortho,:] = from_codon_to_CAI(codons_list, col_cur_ortho, cur_ortho_weights)\n",
    "            ver_CAI[:,cur_ortho,:] = from_codon_to_CAI(codons_list, ver_cur_ortho, cur_ortho_weights)\n",
    "\n",
    "        '''Calculate average CAI per position:\n",
    "        Average over orthologs to get the mean cai per position, for both original and randomized data. '''\n",
    "        mean_cai_orig = np.nanmean(orig_CAI, axis = 0)\n",
    "        mean_cai_col = np.nanmean(col_CAI, axis = 1)\n",
    "        mean_cai_ver = np.nanmean(ver_CAI, axis = 1)\n",
    "        \n",
    "        '''Create a mask of valid positions: positions for which less than 50% of orthologs have dels and \n",
    "        whose variance between randomizations is not practically zero'''\n",
    "        percent_nans = np.sum(np.isnan(orig_CAI),axis = 0) / num_orthologs\n",
    "        allowed_nans = 0.5\n",
    "        good_positions_mask1 = percent_nans < allowed_nans # a \"good position\" is defined as a position where less than \"allowed nans\" of orthologs had a deletion\n",
    "        \n",
    "        good_positions_mask_2v = np.round(np.std(mean_cai_ver,axis = 0),5) != 0   #find positions where the variation between the randomizations is larger than 0.00005\n",
    "        good_positions_mask_2c = np.round(np.std(mean_cai_col,axis = 0),5) != 0   #find positions where the variation between the randomizations is larger than 0.00005\n",
    "        \n",
    "        good_positions_mask_v = good_positions_mask1 & good_positions_mask_2v\n",
    "        good_positions_mask_c = good_positions_mask1 & good_positions_mask_2c\n",
    "\n",
    "        ''' Get the z-score and p-value for each position '''\n",
    "        res_ver = get_scores(mean_cai_orig, mean_cai_ver, good_positions_mask_v) \n",
    "        res_col = get_scores(mean_cai_orig, mean_cai_col, good_positions_mask_c)\n",
    "\n",
    "        '''save scores:'''\n",
    "        #column\n",
    "        path = f\"../Results/AllGenes/CAI/z-scores/{gene}_column.pickle\"\n",
    "        with open(path, 'wb') as handle:\n",
    "            pickle.dump(res_col, handle)\n",
    "        #vertical\n",
    "        path = f\"../Results/AllGenes/CAI/z-scores/{gene}_vertical.pickle\"\n",
    "        with open(path, 'wb') as handle:\n",
    "            pickle.dump(res_ver, handle)\n",
    "  \n",
    "    except Exception as e:\n",
    "        error_path = f\"../Results/AllGenes/CAI/z-scores/error_z_scores.txt\"\n",
    "        file_object = open(error_path, 'a')\n",
    "        file_object.write(f\"gene {gene} failed with error: {e}\")\n",
    "        file_object.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a5bce-8f12-41ed-8518-d5d66b61d18e",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44db33b0-fdd7-4452-8486-3982f5bfa7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to be used by all genes\n",
    "cai_path = '../Results/CAI_tables/' #CAI per codon dictionay for each organism in our orthologs group\n",
    "CAI_all_organisms = create_CAI_dict_all_organisms(cai_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2307c3de-a0d2-4581-abcf-94ef0563d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' split genes to batchs to improve preformance '''\n",
    "genes_list = pd.read_pickle(\"../co_trans_data/genes_with_rands.pickle\") #genes with rands: a pickle with a list of genes for which we have randomizations and didnt fall in the pipeline\n",
    "\n",
    "#Create 30 batches of genes\n",
    "num_wanted_cpus = 30\n",
    "num_genes = len(genes_list)\n",
    "num_genes_per_batch = int(np.round(num_genes /num_wanted_cpus))\n",
    "batches_of_genes = [genes_list[x:x+num_genes_per_batch] for x in range(0, num_genes, num_genes_per_batch)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3cc39-ce15-4339-a84e-7241365e4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' run in parallel ''' \n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for single_batch in batches_of_genes:\n",
    "        futures.append(executor.submit(do_for_single_batch, single_batch_genes = single_batch, num_randomizations = 101))\n",
    "                      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
