{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we find positions with significantly low/high MFE. \n",
    "First, we need to take the mfe per window scores that we have from \"mfe_windows.ipynb\" and map them to positions in the CDS. \n",
    "This is elaboratly explained in the description of the function \"calc_mfe_per_position\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from Bio import AlignIO\n",
    "import concurrent.futures\n",
    "import statsmodels.stats.multitest\n",
    "import scipy.stats\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nansumwrapper(vectors_to_sum, axis:int) -> float:\n",
    "    '''\n",
    "    We need to sum the columns of \"mfe_windows_padded\" and its shifted versions. We need for\n",
    "    nan + nan to be equal nan but the numpy function np.nansum returns zero in that case. \n",
    "    (This is bad for us because a 0 score means positions with weak folding and this is not true for\n",
    "    nan positions). So, we create a wrapper for that function, that returns a nan if all the values\n",
    "    summed are equal nan.\n",
    "    '''\n",
    "    nan_positions = np.isnan(vectors_to_sum).all(axis) #positions where both vectors that are being summed are equal nan. \n",
    "    res = np.nansum(vectors_to_sum, axis) #the result of the summation, there are zeros where only nans are summed\n",
    "    res[nan_positions] = np.nan #insert nans in those locations\n",
    "    return(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mfe_per_position(mfe_windows:np.ndarray, data:str, padding_amount:int, window_size:int) -> np.ndarray:\n",
    "    '''\n",
    "    In this function we perform the transition from mfe of windows to mfe of positions in the cds. The mfe score of a position \n",
    "    is the mean of the scores of all windows the position is in. We are creating \"window_size - 1\" shifted versions of the mfe_window\n",
    "    results vector and so we take the mean of the windows in a vectorized manner. \n",
    "\n",
    "    example: \n",
    "    let's say we have an original sequence of length 10, and the window size is 4. Then the number of windows is 7\n",
    "    (length_cds - window_size + 1). \n",
    "    let's say the window scores are these: mfe_windows = [-1,-3,-2,-2,-4,-3,-2]\n",
    "    according to the rule, the results should be:\n",
    "    mfe_position_0 = mean(mfe_windows[0]) = -1 (position 0 is only in window 0)\n",
    "    mfe_position_1 = mean(mfe_windows[0,1]) = ((-1-3)/2) = -2\n",
    "    mfe_position_2 = mean(mfe_windows[0,1,2]) = ((-1-3-2)/3) = -2\n",
    "    mfe_position_3 = mean(mfe_windows[0,1,2,3]) = ((-1-3-2-2)/4) = -2\n",
    "    mfe_position_4 = mean(mfe_windows[1,2,3,4]) = ((-3-2-2-4)/4) = -2.75\n",
    "    mfe_position_5 = mean((mfe_windows[2,3,4,5])) = ((-2-2-4-3)/4) = -2.75\n",
    "    mfe_position_6 = mean((mfe_windows[3,4,5,6])) = ((-2-4-3-2)/4) = -2.75\n",
    "    mfe_position_7 = mean((mfe_windows[4,5,6])) = ((-4,-3,-2)/3) = -3\n",
    "    mfe_position_8 = mean((mfe_windows[5,6])) = ((-3,-2)/2) = -2.5\n",
    "    mfe_position_9 = mean((mfe_windows[6])) = -2\n",
    "\n",
    "    let's show that we are getting the same result using our method (but fast...)\n",
    "    pad the mfe_windows vector: add nans to the end to make it in the length of \"num_positions\"\n",
    "    mfe_windows = [-1,-3,-2,-2,-4,-3,-2] (length 7) -> mfe_windows_padded = [-1,-3,-2,-2,-4,-3,-2,nan,nan,nan] (length 10)\n",
    "    shifted_1 = [nan,-1,-3,-2,-2,-4,-3,-2,nan,nan] (one score was moved from the end to the beginning)\n",
    "    shifted_2 = [nan,nan,-1,-3,-2,-2,-4,-3,-2,nan] (two scores were moved from the end to the beginning)\n",
    "    shifted_3 = [nan,nan,nan,-1,-3,-2,-2,-4,-3,-2] \n",
    "    (how many shifted versions do we need? window_size -1)\n",
    "\n",
    "    now, we are summing mfe_windows,shifted_1,shifted_2 and shifted_3. \n",
    "    (in the code we do this one after the other so we dont have to keep all the matrices, because this is memory inefficient)\n",
    "    We take the average of each column:\n",
    "    mean_col0 = mean([-1,nan,nan,nan]) = -1\n",
    "    mean_col1 = mean([-3,-1,nan,nan]) = -2\n",
    "    mean_col2 = mean([-2,-3,-1,nan]) = -2\n",
    "    mean_col3 = mean([-2,-2,-3,-1]) = -2\n",
    "    mean_col4 = mean([-4,-2,-2,-3]) = -2.75\n",
    "    mean_col5 = mean([-3,-4,-2,-2]) = -2.75\n",
    "    mean_col6 = mean([-2,-3,-4,-2]) = -2.75\n",
    "    mean_col7 = mean([nan,-2,-3,-4]) = -3\n",
    "    mean_col8 = mean([nan,nan,-2,-3]) = -2.5\n",
    "    mean_col9 = mean([nan,nan,nan,-2]) = -2\n",
    "    (Notice that we are summing over the not-nan positions so we have to keep track of the amount of nans per position\n",
    "    to calculate the mean). \n",
    "\n",
    "    we can see that we got the same results! but the second way can be vectorized so we will use it.\n",
    "    '''\n",
    "    #perform the padding\n",
    "    if data == 'original':\n",
    "        mfe_windows_padded = np.pad(mfe_windows, ((0,0),(0, padding_amount)), 'constant', constant_values=(np.nan))\n",
    "    else:\n",
    "        mfe_windows_padded = np.pad(mfe_windows, ((0,0),(0,0),(0, padding_amount)), 'constant', constant_values=(np.nan))\n",
    "    #perform the averaging. #each time, we shift and sum. later we devide by the number of windows the position was in. \n",
    "    mfe_per_position = mfe_windows_padded.copy() #initialize\n",
    "    nans_per_position = np.isnan(mfe_windows_padded).astype(int) #keep track of nans in order to devide later by num of non-nan positions\n",
    "    axis = 1 if data == 'original' else 2\n",
    "    for i in range(1,window_size):\n",
    "        shifted = np.roll(mfe_windows_padded, i, axis = axis) #create the next shifted version\n",
    "        nans_per_position = nans_per_position + np.isnan(shifted).astype(int) #where are the nans in this shifted version? \n",
    "        mfe_per_position = nansumwrapper([mfe_per_position, shifted], axis = 0)#sum the prior columns of the prior result with this shifted version\n",
    "\n",
    "    not_nans_per_position = window_size - nans_per_position #for each column, how many elements were not equal to nan?\n",
    "    mfe_per_position = mfe_per_position/not_nans_per_position #for each column, devide by the number of not nan elements\n",
    "    return(mfe_per_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_mfe_tails(mfe_scores_cur_ortholog_orig: np.ndarray, mfe_scores_cur_ortholog_col: np.ndarray, mfe_scores_cur_ortholog_ver:np.ndarray, cur_ortholog_length:int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    Our mfe scores are in shape (num_orthologs, max_num_positions) where max_num_positions is the number of positions of\n",
    "    the longest ortholog, meaning the score vectors of the other orthologs have a nan \"tail\". We now want to remove that nan\n",
    "    tail so the number of mfe scores of an ortholog will be equal to the amount of positions of the cds (at the next step,\n",
    "    we will take these scores and align them to the MSA, meaning, insert deletions between the scores according to the MSA.)\n",
    "\n",
    "    This funciton receives:\n",
    "    \"mfe_scores_cur_ortholog_orig\" : the true mfe scores per position of a single ortholog (1D numpy array in shape (1, max_num_positions))\n",
    "    \"mfe_scores_cur_ortholog_col\" : the mfe scores per position of the column randomization of a single ortholog (2D numpy array in shape (num_randomizations,max_num_positions))\n",
    "    \"mfe_scores_cur_ortholog_ver\" : the mfe scores per position of the vertical randomization of a single ortholog (2D numpy array in shape (num_randomizations,max_num_positions))\n",
    "    \"cur_ortholog_length\" : the length of the cds of this ortholog\n",
    "    and returns:\n",
    "    \"mfe_scores_cur_ortholog_orig_trimmed\": the true mfe scores per position of a single ortholog, removed of the nan tail (1D numpy array in shape (1,cur_ortholog_length))\n",
    "    \"mfe_scores_cur_ortholog_col_trimmed\": the mfe scores per position of the column randomizations of a single ortholog, removed of the nan tail (2D numpy array in shape (num_randomizations,cur_ortholog_length))\n",
    "    \"mfe_scores_cur_ortholog_ver_trimmed\": the mfe scores per position of the vertical randomizations of a single ortholog, removed of the nan tail (2D numpy array in shape (num_randomizations,cur_ortholog_length))\n",
    "    '''\n",
    "    indices_to_delete = np.arange(cur_ortholog_length,len(mfe_scores_cur_ortholog_orig)) #all indices larger than the length of the cur_ortholog\n",
    "    assert(np.all(np.isnan(mfe_scores_cur_ortholog_orig[indices_to_delete]) == True)) #make sure we are deleting only nans and not a true mfe score  \n",
    "    #delete for original:\n",
    "    mfe_scores_cur_ortholog_orig = np.delete(mfe_scores_cur_ortholog_orig,indices_to_delete) #deleting these indices\n",
    "    #delete for column rands:\n",
    "    mfe_scores_cur_ortholog_col = np.delete(mfe_scores_cur_ortholog_col, indices_to_delete, axis = 1) \n",
    "    #delete for ver rands:\n",
    "    mfe_scores_cur_ortholog_ver = np.delete(mfe_scores_cur_ortholog_ver, indices_to_delete, axis = 1) \n",
    "    return(mfe_scores_cur_ortholog_orig,mfe_scores_cur_ortholog_col,mfe_scores_cur_ortholog_ver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfe_scores(gene:str, mean_mfe_orig:np.ndarray, mean_mfe_col:np.ndarray, mean_mfe_ver:np.ndarray) -> None:\n",
    "    ''' \n",
    "    Save the scores to a the results folder\n",
    "    '''\n",
    "    output_path = f\"../Results//mfe_scores/original/{gene}.pickle\"\n",
    "    with open(output_path, 'wb') as handle:\n",
    "        pickle.dump(mean_mfe_orig, handle)\n",
    "    !gzip $output_path\n",
    "    \n",
    "    output_path = f\"../Results//mfe_scores/column/{gene}.pickle\"\n",
    "    with open(output_path, 'wb') as handle:\n",
    "        pickle.dump(mean_mfe_col, handle)\n",
    "    !gzip $output_path\n",
    "        \n",
    "    output_path = f\"../Results//mfe_scores/vertical/{gene}.pickle\"\n",
    "    with open(output_path, 'wb') as handle:\n",
    "        pickle.dump(mean_mfe_ver, handle)\n",
    "    !gzip $output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(original_scores: np.ndarray, random_scores: np.ndarray, good_mask: np.ndarray) -> pd.DataFrame:\n",
    "    ''' \n",
    "    This functions returns the z-scores and respective p-values for each position. \n",
    "    '''\n",
    "    #get the parameters of the normal distribution\n",
    "    miu = np.mean(random_scores, axis = 0)\n",
    "    sigma = np.std(random_scores, axis = 0)\n",
    "    \n",
    "    #get the z-scores\n",
    "    z_scores = (original_scores - miu) / sigma\n",
    "    \n",
    "    #get the one-sided p-values \n",
    "    p_vals = scipy.stats.norm.sf(abs(z_scores))\n",
    "    # correct FDR. we have nan values (for positions where the randomizations are the same and so sigma = 0) and the FDR correction function doesnt work with nans, \n",
    "    # so we will use a mask\n",
    "    mask = np.isfinite(p_vals) #mask contains \"True\" only the non-nan positions\n",
    "    pval_corrected = np.empty(p_vals.shape) #initilize the final result in the right dimensions\n",
    "    pval_corrected.fill(np.nan) #fill it with nans\n",
    "    pval_corrected[mask] = statsmodels.stats.multitest.multipletests(p_vals[mask],method='fdr_bh')[1] #insert the corrected p-vals at the non-nan positions\n",
    "    \n",
    "    res_df = pd.DataFrame(columns = [\"z-score\", \"p-value\", \"corrected p-value\", \"good_position\"])\n",
    "    res_df.index.name='position of CDS'\n",
    "    res_df[\"z-score\"] = z_scores\n",
    "    res_df[\"p-value\"] = p_vals\n",
    "    res_df[\"corrected p-value\"] = pval_corrected\n",
    "    res_df[\"good_position\"] = good_mask\n",
    "\n",
    "    return(res_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_pipline_single_gene(gene:str,window_size:int = 39) -> None:\n",
    "    ''' \n",
    "    This function gets the mfe z-scores per CDS position of a single gene\n",
    "    '''\n",
    "    #try:\n",
    "        \n",
    "    ################# (1) window mfe scores to position mfe scores ################################################\n",
    "    '''Download the mfe per window results'''\n",
    "    mfe_orig = pd.read_pickle(f\"../Results/window_mfe_scores/original/{gene}.pickle.gz\")\n",
    "    mfe_col = pd.read_pickle(f\"../Results/window_mfe_scores/column/{gene}.pickle.gz\")\n",
    "    mfe_ver = pd.read_pickle(f\"../Results/window_mfe_scores/vertical/{gene}.pickle.gz\")\n",
    "    '''Define needed parameters'''\n",
    "    num_randomizations,num_orthologs, max_num_windows  = mfe_col.shape #get the dimentions of the results of the gene\n",
    "    max_num_positions = max_num_windows + window_size -1 #the num of cds positions (not including the stop codon) of the ortholog with the longest cds\n",
    "    pad = max_num_positions -  max_num_windows #the needed amount of nucleotides to pad the input of the moving average function to be the size of the output\n",
    "\n",
    "    '''From mfe per window to mfe per position, for the original and randomized CDSs'''\n",
    "    mfe_per_position_orig = calc_mfe_per_position(mfe_orig, data = 'original', padding_amount = pad, window_size = window_size)\n",
    "    del mfe_orig\n",
    "    #print(\"done calculating mfe per position for orig\")\n",
    "    mfe_per_position_col = calc_mfe_per_position(mfe_col, data = 'column', padding_amount = pad, window_size = window_size)\n",
    "    #print(\"done calculating mfe per position for col\")\n",
    "    del mfe_col\n",
    "    mfe_per_position_ver = calc_mfe_per_position(mfe_ver, data = 'vertical', padding_amount = pad, window_size = window_size)\n",
    "    #print(\"done calculating mfe per position for ver\")\n",
    "    del mfe_ver\n",
    "\n",
    "    ################# (2) align to MSAs ################################################\n",
    "    '''\n",
    "    At this point we have our mfe scores per cds position, but the positions are not aligned to the MSA. \n",
    "    We have to align to the MSA in order to compare the mfe scores in each position. \n",
    "    We need to insert deletions where they occur in the MSA.\n",
    "    We can get the deletion positions from the original MSA (the deletion positions are the same for the randomizations and the original)\n",
    "    '''\n",
    "\n",
    "    '''download the MSA in nucleotides'''\n",
    "    path_NT = f\"../co_trans_data/orthologs/nt_after_msa/{gene}.fasta.gz\" #MSA in nucleotides\n",
    "    local_path = f\"./{gene}.fasta\"\n",
    "    !gzip -c -d $path_NT > $local_path #unzip without erasing the zipped version\n",
    "    alignment_NT = AlignIO.read(local_path, \"fasta\")\n",
    "    os.remove(local_path)\n",
    "    '''Find all positions of deletions. Use a mask to place the mfe scores in the non-deletions positions.'''\n",
    "    nuc_array = np.array(alignment_NT).astype('object') # an array where each column is a nucleotide position and each row is a cds of an ortholog\n",
    "    non_deletions_mask = nuc_array != '-' #Get all locations of non-deletions in the MSA. \n",
    "    human_del_locs = np.where(~non_deletions_mask[0,:])[0] #we need the positions of the human ortholog deletions later on\n",
    "\n",
    "    nuc_array_col = np.tile(nuc_array,(num_randomizations,1,1)) #duplicating \"nuc_array\" \"num_randomizations\" times for our column randomizations. We care only about the deletions positions here. \n",
    "    nuc_array_ver = np.tile(nuc_array,(num_randomizations,1,1)) #duplicating \"nuc_array\" \"num_randomizations\" times for our vertical randomizations. We care only about the deletions positions here. \n",
    "\n",
    "    '''Now we will iterate over the sequences of the original and random MSAs. For each sequence, we have an mfe scores\n",
    "    per position vector in the size of (1, max_num_positions). For each sequence we will take the\n",
    "    mfe results and we place them at the non-deletion locations according to our mask. For example, if the current ortholog has \n",
    "    12 positions and the longest one has 15 positions, the mfe_scores of our ortholog could look like\n",
    "    [-1,-2,-1,-3,-6,-3,-6,-2,-2,-1,-1,-5,nan,nan,nan] -> 12 scores, 3 nans. Our original ortholog looks like : \"ATGAAGCCTACC\"\n",
    "    The MSA could look like: \"ATGAAGCCT---ACC---\" -> 12 non-deletion positions (has to be the same as the number of scores in our vector)\n",
    "    Then, we want to take our results, discard the nan tail and align them to the MSA. \"plant\" the deletions among them so the\n",
    "    final result looks like this : [-1,-2,-1,-3,-6,-3,-6,-2,-2,-,-,-,-1,-1,-5,-,-,-]\n",
    "    '''\n",
    "\n",
    "    lengths_dict = pd.read_pickle(f\"../co_trans_data/cds_lengths/{gene}.pickle\") #a dictionary containing the lengths of the cdss\n",
    "\n",
    "    #trim nan tails & insert deletions to align to the msa\n",
    "    for cur_ortholog in range(num_orthologs):\n",
    "        trimmed_orig, trimmed_col, trimmed_ver = trim_mfe_tails(mfe_per_position_orig[cur_ortholog,:],mfe_per_position_col[:,cur_ortholog,:],mfe_per_position_ver[:,cur_ortholog,:],lengths_dict[cur_ortholog])\n",
    "        cur_mask = non_deletions_mask[cur_ortholog,:] #get positions of non deletions of this ortholog in the MSA \n",
    "        #inserting the mfe results to the non-deletion positions in the MSA:\n",
    "        nuc_array[cur_ortholog,cur_mask] = trimmed_orig #original\n",
    "        nuc_array_col[:,cur_ortholog,cur_mask] = trimmed_col #column rands\n",
    "        nuc_array_ver[:,cur_ortholog,cur_mask] = trimmed_ver #vertical rands\n",
    "    #print(\"done aligning to MSA\")\n",
    "    ################# (3) get average mfe per position ################################################\n",
    "    '''Average over orthologs to get the mean mfe per position, for both original and randomized data.''' \n",
    "    '''remove deletion positions in the human ortholog (we want the final scores to regard only positions that exist\n",
    "    in the human ortholog)'''\n",
    "    nuc_array = np.delete(nuc_array, human_del_locs, axis = 1) #nuc_array is a 2D matrix holding the MSA in nucleotides\n",
    "    nuc_array_col = np.delete(nuc_array_col, human_del_locs, axis = 2) \n",
    "    nuc_array_ver = np.delete(nuc_array_ver, human_del_locs, axis = 2) \n",
    "\n",
    "    '''change deletions (all other orthologs other than the human one still could have deletions) to nans for mean computation'''\n",
    "    nuc_array[nuc_array == '-'], nuc_array_col[nuc_array_col == '-'], nuc_array_ver[nuc_array_ver == '-'] = np.nan, np.nan, np.nan  \n",
    "\n",
    "    '''change type of data to float'''\n",
    "    nuc_array = nuc_array.astype(float) #now that we dont have deletions we can change datatype to float and find nan locations\n",
    "    nuc_array_col = nuc_array_col.astype(float)\n",
    "    nuc_array_ver = nuc_array_ver.astype(float)\n",
    "\n",
    "    '''calculate the mean of each position'''\n",
    "    mean_mfe_orig, mean_mfe_col, mean_mfe_ver  = np.nanmean(nuc_array,axis = 0), np.nanmean(nuc_array_col,axis = 1), np.nanmean(nuc_array_ver,axis = 1) \n",
    "\n",
    "    '''there are positions that are found significant because of rounding issues-'''\n",
    "    mean_mfe_orig, mean_mfe_col, mean_mfe_ver = np.round(mean_mfe_orig,decimals=8), np.round(mean_mfe_col,decimals=8), np.round(mean_mfe_ver,decimals=8)\n",
    "\n",
    "    save_mfe_scores(gene, mean_mfe_orig, mean_mfe_col, mean_mfe_ver)\n",
    "    #print(\"done saving the mfe scores\")\n",
    "    ################# (4) find significantly weak and strong positions ################################################\n",
    "    '''First we will create a mask of valid positions:\n",
    "    -Find positions with more than \"allowed_nans\" deletions (among orthologs) and do not use them (unreliable data - even if they are \n",
    "    found as significant- ignore them). We do not want to delete them because we want to keep the current length of the human cds.''' \n",
    "    percent_nans = np.sum(np.isnan(nuc_array),axis = 0) / num_orthologs\n",
    "    del nuc_array, nuc_array_col, nuc_array_ver\n",
    "    allowed_nans = 0.5\n",
    "    good_positions_mask1 = percent_nans < allowed_nans # a \"good position\" is defined as a position where less than \"allowed nans\" of orthologs had a deletion\n",
    "\n",
    "    '''we also do not want to consider positions with STD == 0 between the randomizations (or practically zero), meaning where all the randomizations are the same. \n",
    "    They could all be slightly bigger/smaller than the randomization because of rounding when averaging and result in a \"significant\" \n",
    "    positions without a real reason.'''\n",
    "    good_positions_mask_2v = np.round(np.std(mean_mfe_ver,axis = 0),5) != 0   #find positions where the variation between the randomizations is larger than 0.00005\n",
    "    good_positions_mask_2c = np.round(np.std(mean_mfe_col,axis = 0),5) != 0   #find positions where the variation between the randomizations is larger than 0.00005\n",
    "\n",
    "    '''Combine both masks to obtain valid positions'''\n",
    "    good_positions_mask_v = good_positions_mask1 & good_positions_mask_2v\n",
    "    good_positions_mask_c = good_positions_mask1 & good_positions_mask_2c\n",
    "\n",
    "    ''' Get the z-score and p-value for each position '''\n",
    "    res_ver = get_scores(mean_mfe_orig, mean_mfe_ver, good_positions_mask_v) \n",
    "    res_col = get_scores(mean_mfe_orig, mean_mfe_col, good_positions_mask_c)\n",
    "\n",
    "    '''save scores:'''\n",
    "    output_path = f\"../Results/z-scores/{gene}_column.pickle\"\n",
    "    with open(output_path, 'wb') as handle:\n",
    "        pickle.dump(res_col, handle)\n",
    "    output_path = f\"../Results/z-scores/{gene}_vertical.pickle\"\n",
    "    with open(output_path, 'wb') as handle:\n",
    "        pickle.dump(res_ver, handle)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         file_object = open('../Results/AllGenes/mfe/errors_mfe_positions.txt', 'a')\n",
    "#         file_object.write(f\"gene {gene} failed with error: {e}\")\n",
    "#         file_object.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get genes to run on\n",
    "mypath = \"../Results/window_mfe_scores/original/\" #all the genes with both column and vertical window scores\n",
    "l=os.listdir(mypath)\n",
    "genes_list=[x.split('.')[0] for x in l] #remove the extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split genes to batchs to improve preformance\n",
    "\n",
    "'''Create batches of genes'''\n",
    "num_wanted_cpus = 30\n",
    "num_genes = len(genes_list)\n",
    "num_genes_per_batch = int(np.round(num_genes /num_wanted_cpus))\n",
    "batches_of_genes = [genes_list[x:x+num_genes_per_batch] for x in range(0, num_genes, num_genes_per_batch)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_for_single_batch(single_batch_genes: list) -> None:\n",
    "    '''\n",
    "    This function calls our main function \"whole_pipline_single_gene\" for batch of genes\n",
    "    '''\n",
    "    for gene in single_batch_genes:\n",
    "        whole_pipline_single_gene(gene)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run in parallel\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for single_batch in batches_of_genes:\n",
    "        futures.append(executor.submit(do_for_single_batch, single_batch_genes = single_batch))\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
